{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "from skimage.transform import resize\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for directory, _, file in os.walk('data/chest_xray/train'):\n",
    "    # cutting the data in half because it takes too long to run these models on the full data\n",
    "    # grayscale to better fit in the RF and SVC models\n",
    "    for f in file[1::2]:\n",
    "        f = os.path.join(directory, f)\n",
    "        img = cv2.imread(f, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            # resizing to (150, 150)\n",
    "            img = resize(img, (150, 150, 1))\n",
    "            img = np.asarray(img)\n",
    "            label=f.split('/')[-2]\n",
    "            X.append(img)\n",
    "            y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for directory, _, file in os.walk('data/chest_xray/test'):\n",
    "    # not cutting this in half because the test set is already small\n",
    "    for f in file[1:]:\n",
    "        f = os.path.join(directory, f)\n",
    "        img = cv2.imread(f, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = resize(img, (150, 150, 1))\n",
    "            img = np.asarray(img)\n",
    "            label=f.split('/')[-2]\n",
    "            X_test.append(img)\n",
    "            y_test.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test = np.asarray(X), np.asarray(X_test)\n",
    "y, y_test = np.asarray(y), np.asarray(y_test)\n",
    "\n",
    "X_rf = []\n",
    "# flattening to a 1 dimensional array to feed into the RF and SVC models\n",
    "for i in range(len(X)):\n",
    "    a = X[i].flatten()\n",
    "    X_rf.append(a)\n",
    "    \n",
    "X_rf = np.asarray(X_rf)\n",
    "\n",
    "X_rf_test = []\n",
    "for i in range(len(X_test)):\n",
    "    a = X_test[i].flatten()\n",
    "    X_rf_test.append(a)\n",
    "    \n",
    "X_rf_test = np.asarray(X_rf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarizing\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 'PNEUMONIA':\n",
    "        y[i] = 1\n",
    "    else:\n",
    "        y[i] = 0\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 'PNEUMONIA':\n",
    "        y_test[i] = 1\n",
    "    else:\n",
    "        y_test[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.94552239 0.67294786]\n"
     ]
    }
   ],
   "source": [
    "# computing class weights. this data is imbalanced\n",
    "from sklearn.utils import class_weight\n",
    "y_labels = np.argmax(y)\n",
    "classweight = class_weight.compute_class_weight('balanced', np.unique(y), y)\n",
    "print(classweight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92758621 0.93210587 0.92165899]\n"
     ]
    }
   ],
   "source": [
    "rf = ensemble.RandomForestClassifier()\n",
    "rf.fit(X_rf, y)\n",
    "print(cross_val_score(rf, X_rf, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7958199356913184\n",
      "true negative: 114\n",
      "false negative: 8\n",
      "true positive: 381\n",
      "false positive: 119\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(X_rf_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
    "print('accuracy: {}\\ntrue negative: {}\\nfalse negative: {}\\ntrue positive: {}\\nfalse positive: {}'\n",
    "      .format(accuracy, tn, fn, tp, fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### employing class weights here, doesn't work, for future work\n",
    "#rf = ensemble.RandomForestClassifier(class_weight = {'0':1.9, '1':.67})\n",
    "#rf.fit(X_rf, y)\n",
    "#print(cross_val_score(rf, X_rf, y))\n",
    "\n",
    "#y_pred = rf.predict(X_rf_test)\n",
    "#tn, fp, fn, tp = confusion_matrix(y_pred, y_test).ravel()\n",
    "#accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
    "#print('accuracy: {}\\ntrue negative: {}\\nfalse negative: {}\\ntrue positive: {}\\nfalse positive: {}'\n",
    "#      .format(accuracy, tn, fn, tp, fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does fairly poorly, giving equally likely true and false negatives. The high accuracy is primarily a result of the unbalanced nature of the samples. Sensitivity is our most important metric, and it, at least, it somewhat low. Oddly, class weight balancing appears to make this model perform worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7427652733118971\n",
      "true negative: 79\n",
      "false negative: 6\n",
      "true positive: 383\n",
      "false positive: 154\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_rf, y)\n",
    "y_pred = svc.predict(X_rf_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
    "print('accuracy: {}\\ntrue negative: {}\\nfalse negative: {}\\ntrue positive: {}\\nfalse positive: {}'\n",
    "      .format(accuracy, tn, fn, tp, fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8263665594855305\n",
      "true negative: 149\n",
      "false negative: 24\n",
      "true positive: 365\n",
      "false positive: 84\n"
     ]
    }
   ],
   "source": [
    "# employing class weights here\n",
    "svc = SVC(class_weight = 'balanced')\n",
    "svc.fit(X_rf, y)\n",
    "y_pred = svc.predict(X_rf_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
    "print('accuracy: {}\\ntrue negative: {}\\nfalse negative: {}\\ntrue positive: {}\\nfalse positive: {}'\n",
    "      .format(accuracy, tn, fn, tp, fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall accuracy is significantly higher, yet sensitivity is lower than the random forest model. Runtime is also significantly longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulating the data to give more robust results\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=180,\n",
    "        width_shift_range=.2,\n",
    "        height_shift_range=.2,\n",
    "        rescale=1/255,\n",
    "        shear_range=.2,\n",
    "        zoom_range=.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, 150, 150)\n",
    "else:\n",
    "    input_shape = (150, 150, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=input_shape))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = datagen.flow_from_directory('data/chest_xray/train',\n",
    "                                   target_size=(150, 150),\n",
    "                                   batch_size=16,\n",
    "                                   class_mode='binary')\n",
    "\n",
    "test = test_datagen.flow_from_directory('data/chest_xray/test',\n",
    "                                   target_size=(150, 150),\n",
    "                                   batch_size=16,\n",
    "                                   class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "326/326 [==============================] - 81s 248ms/step - loss: 4.0959 - acc: 0.7410 - val_loss: 5.9784 - val_acc: 0.6250\n",
      "Epoch 2/5\n",
      "326/326 [==============================] - 80s 244ms/step - loss: 4.0987 - acc: 0.7429 - val_loss: 5.9784 - val_acc: 0.6250\n",
      "Epoch 3/5\n",
      "326/326 [==============================] - 80s 244ms/step - loss: 4.0987 - acc: 0.7429 - val_loss: 5.9784 - val_acc: 0.6250\n",
      "Epoch 4/5\n",
      "326/326 [==============================] - 80s 246ms/step - loss: 4.0987 - acc: 0.7429 - val_loss: 5.9784 - val_acc: 0.6250\n",
      "Epoch 5/5\n",
      "326/326 [==============================] - 81s 248ms/step - loss: 4.0987 - acc: 0.7429 - val_loss: 5.9784 - val_acc: 0.6250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x155b9d9d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train,\n",
    "                   steps_per_epoch=5216 // 16,\n",
    "                   epochs = 5,\n",
    "                   validation_data = test,\n",
    "                   validation_steps=624 // 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "39/39 [==============================] - 5s 134ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.978394532815004, 0.625]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inputting test images in color and for the last model\n",
    "X = []\n",
    "y = []\n",
    "for directory, _, file in os.walk('data/chest_xray/train'):\n",
    "    for f in file[1:]:\n",
    "        f = os.path.join(directory, f)\n",
    "        img = cv2.imread(f, cv2.IMREAD_COLOR)\n",
    "        if img is not None:\n",
    "            img = resize(img, (150, 150, 3))\n",
    "            img = np.asarray(img)\n",
    "            label=f.split('/')[-2]\n",
    "            X.append(img)\n",
    "            y.append(label)\n",
    "            \n",
    "X_test = []\n",
    "y_test = []\n",
    "for directory, _, file in os.walk('data/chest_xray/test'):\n",
    "    for f in file[1:]:\n",
    "        f = os.path.join(directory, f)\n",
    "        img = cv2.imread(f, cv2.IMREAD_COLOR)\n",
    "        if img is not None:\n",
    "            img = resize(img, (150, 150, 3))\n",
    "            img = np.asarray(img)\n",
    "            label=f.split('/')[-2]\n",
    "            X_test.append(img)\n",
    "            y_test.append(label)\n",
    "            \n",
    "X, X_test = np.asarray(X), np.asarray(X_test)\n",
    "y, y_test = np.asarray(y), np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    if y[i] == 'PNEUMONIA':\n",
    "        y[i] = 1\n",
    "    else:\n",
    "        y[i] = 0\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 'PNEUMONIA':\n",
    "        y_test[i] = 1\n",
    "    else:\n",
    "        y_test[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6254019292604501\n",
      "true negative: 0\n",
      "false negative: 0\n",
      "true positive: 389\n",
      "false positive: 233\n"
     ]
    }
   ],
   "source": [
    "y = [int(i) for i in y]\n",
    "y_test = [int(i) for i in y_test]\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
    "print('accuracy: {}\\ntrue negative: {}\\nfalse negative: {}\\ntrue positive: {}\\nfalse positive: {}'\n",
    "      .format(accuracy, tn, fn, tp, fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dense network has one 64 feature layer and drops out half the data to avoid overfitting. This model only ended up predicting 1s, so we'll weight it as well and see if that changes anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=input_shape))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              weighted_metrics=['categorical_accuracy'],\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "326/326 [==============================] - 85s 261ms/step - loss: 4.0926 - acc: 0.7418 - weighted_categorical_accuracy: 1.0000 - val_loss: 5.9784 - val_acc: 0.6250 - val_weighted_categorical_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "326/326 [==============================] - 77s 237ms/step - loss: 4.0997 - acc: 0.7427 - weighted_categorical_accuracy: 1.0000 - val_loss: 5.9784 - val_acc: 0.6250 - val_weighted_categorical_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "326/326 [==============================] - 76s 234ms/step - loss: 4.0987 - acc: 0.7429 - weighted_categorical_accuracy: 1.0000 - val_loss: 5.9784 - val_acc: 0.6250 - val_weighted_categorical_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "326/326 [==============================] - 77s 237ms/step - loss: 4.0987 - acc: 0.7429 - weighted_categorical_accuracy: 1.0000 - val_loss: 5.9784 - val_acc: 0.6250 - val_weighted_categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "326/326 [==============================] - 76s 232ms/step - loss: 4.0987 - acc: 0.7429 - weighted_categorical_accuracy: 1.0000 - val_loss: 5.9784 - val_acc: 0.6250 - val_weighted_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1563dfbd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train,\n",
    "                   steps_per_epoch=5216 // 16,\n",
    "                   epochs = 5,\n",
    "                   validation_data = test,\n",
    "                   validation_steps=624 // 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6254019292604501\n",
      "true negative: 0\n",
      "false negative: 0\n",
      "true positive: 389\n",
      "false positive: 233\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
    "print('accuracy: {}\\ntrue negative: {}\\nfalse negative: {}\\ntrue positive: {}\\nfalse positive: {}'\n",
    "      .format(accuracy, tn, fn, tp, fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It has fairly miserable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "326/326 [==============================] - 96s 295ms/step - loss: 0.5552 - acc: 0.7412 - val_loss: 0.7706 - val_acc: 0.6250\n",
      "Epoch 2/30\n",
      "326/326 [==============================] - 94s 288ms/step - loss: 0.4781 - acc: 0.7429 - val_loss: 0.8226 - val_acc: 0.6250\n",
      "Epoch 3/30\n",
      "326/326 [==============================] - 97s 298ms/step - loss: 0.4685 - acc: 0.7442 - val_loss: 1.0390 - val_acc: 0.6250\n",
      "Epoch 4/30\n",
      "326/326 [==============================] - 98s 302ms/step - loss: 0.4546 - acc: 0.7429 - val_loss: 0.9185 - val_acc: 0.6250\n",
      "Epoch 5/30\n",
      "326/326 [==============================] - 98s 301ms/step - loss: 0.4482 - acc: 0.7400 - val_loss: 0.7070 - val_acc: 0.6250\n",
      "Epoch 6/30\n",
      "326/326 [==============================] - 98s 301ms/step - loss: 0.4438 - acc: 0.7456 - val_loss: 0.6843 - val_acc: 0.6266\n",
      "Epoch 7/30\n",
      "326/326 [==============================] - 98s 300ms/step - loss: 0.4430 - acc: 0.7496 - val_loss: 0.8373 - val_acc: 0.6234\n",
      "Epoch 8/30\n",
      "326/326 [==============================] - 98s 299ms/step - loss: 0.4234 - acc: 0.7665 - val_loss: 0.6838 - val_acc: 0.6667\n",
      "Epoch 9/30\n",
      "326/326 [==============================] - 98s 300ms/step - loss: 0.4197 - acc: 0.7628 - val_loss: 0.7050 - val_acc: 0.6522\n",
      "Epoch 10/30\n",
      "326/326 [==============================] - 99s 303ms/step - loss: 0.4121 - acc: 0.7765 - val_loss: 0.6064 - val_acc: 0.7436\n",
      "Epoch 11/30\n",
      "326/326 [==============================] - 97s 299ms/step - loss: 0.3965 - acc: 0.7943 - val_loss: 0.5699 - val_acc: 0.7067\n",
      "Epoch 12/30\n",
      "326/326 [==============================] - 97s 299ms/step - loss: 0.3880 - acc: 0.8284 - val_loss: 0.6826 - val_acc: 0.7356\n",
      "Epoch 13/30\n",
      "326/326 [==============================] - 97s 298ms/step - loss: 0.3626 - acc: 0.8405 - val_loss: 0.5660 - val_acc: 0.6971\n",
      "Epoch 14/30\n",
      "326/326 [==============================] - 98s 300ms/step - loss: 0.3524 - acc: 0.8457 - val_loss: 0.5953 - val_acc: 0.6603\n",
      "Epoch 15/30\n",
      "326/326 [==============================] - 98s 302ms/step - loss: 0.3470 - acc: 0.8539 - val_loss: 0.5406 - val_acc: 0.7580\n",
      "Epoch 16/30\n",
      "326/326 [==============================] - 100s 305ms/step - loss: 0.3289 - acc: 0.8652 - val_loss: 0.5293 - val_acc: 0.6971\n",
      "Epoch 17/30\n",
      "326/326 [==============================] - 2052s 6s/step - loss: 0.3085 - acc: 0.8771 - val_loss: 0.5901 - val_acc: 0.7484\n",
      "Epoch 18/30\n",
      "326/326 [==============================] - 103s 316ms/step - loss: 0.3007 - acc: 0.8731 - val_loss: 0.6693 - val_acc: 0.7372\n",
      "Epoch 19/30\n",
      "326/326 [==============================] - 102s 314ms/step - loss: 0.3086 - acc: 0.8660 - val_loss: 0.5034 - val_acc: 0.7853\n",
      "Epoch 20/30\n",
      "326/326 [==============================] - 102s 312ms/step - loss: 0.2820 - acc: 0.8850 - val_loss: 0.5782 - val_acc: 0.8109\n",
      "Epoch 21/30\n",
      "326/326 [==============================] - 104s 318ms/step - loss: 0.2796 - acc: 0.8834 - val_loss: 0.3978 - val_acc: 0.7965\n",
      "Epoch 22/30\n",
      "326/326 [==============================] - 103s 315ms/step - loss: 0.2649 - acc: 0.8890 - val_loss: 0.4694 - val_acc: 0.8013\n",
      "Epoch 23/30\n",
      "326/326 [==============================] - 103s 315ms/step - loss: 0.2561 - acc: 0.8917 - val_loss: 0.4642 - val_acc: 0.7853\n",
      "Epoch 24/30\n",
      "326/326 [==============================] - 102s 314ms/step - loss: 0.2730 - acc: 0.8855 - val_loss: 0.4562 - val_acc: 0.7548\n",
      "Epoch 25/30\n",
      "326/326 [==============================] - 103s 315ms/step - loss: 0.2674 - acc: 0.8873 - val_loss: 0.4683 - val_acc: 0.7981\n",
      "Epoch 26/30\n",
      "326/326 [==============================] - 102s 313ms/step - loss: 0.2406 - acc: 0.8986 - val_loss: 0.4625 - val_acc: 0.7949\n",
      "Epoch 27/30\n",
      "326/326 [==============================] - 2074s 6s/step - loss: 0.2433 - acc: 0.8970 - val_loss: 0.3914 - val_acc: 0.8429\n",
      "Epoch 28/30\n",
      "326/326 [==============================] - 178s 545ms/step - loss: 0.2580 - acc: 0.8926 - val_loss: 0.4290 - val_acc: 0.7965\n",
      "Epoch 29/30\n",
      "326/326 [==============================] - 1033s 3s/step - loss: 0.2436 - acc: 0.9003 - val_loss: 0.4064 - val_acc: 0.8494\n",
      "Epoch 30/30\n",
      "326/326 [==============================] - 105s 322ms/step - loss: 0.2424 - acc: 0.8974 - val_loss: 0.3710 - val_acc: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11032f490>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train,\n",
    "                   steps_per_epoch=5216 // 16,\n",
    "                   epochs = 30,\n",
    "                   validation_data = test,\n",
    "                   validation_steps=624 // 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8102893890675241\n",
      "true negative: 190\n",
      "false negative: 75\n",
      "true positive: 314\n",
      "false positive: 43\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# binarizing output\n",
    "y_binary_pred = []\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] >= .5:\n",
    "        y_binary_pred.append(1)\n",
    "    else:\n",
    "        y_binary_pred.append(0)\n",
    "        \n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_binary_pred).ravel()\n",
    "accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
    "print('accuracy: {}\\ntrue negative: {}\\nfalse negative: {}\\ntrue positive: {}\\nfalse positive: {}'\n",
    "      .format(accuracy, tn, fn, tp, fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our CNN model uses three convolutional neural networks chained together with pooling layers, and round it out with the same two dense layers around a 50% dropout as before. Performance is better, but still falls short of ever being useful. Weighting it (weighted modeled not included) didn't appear to help, but the unweighted model has a fairly low number of errors, especially type II errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 16386     \n",
      "=================================================================\n",
      "Total params: 14,731,074\n",
      "Trainable params: 14,731,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = VGG16(include_top=False, input_shape=(150, 150, 3))\n",
    "# binarizing our output\n",
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=model.input, outputs=x)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezing the top 18 layers to expedite this training\n",
    "for layer in model.layers[0:18]:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 16386     \n",
      "=================================================================\n",
      "Total params: 14,731,074\n",
      "Trainable params: 16,386\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5214/5214 [==============================] - 186s 36ms/step - loss: 0.3704 - acc: 0.8400\n",
      "Epoch 2/20\n",
      "5214/5214 [==============================] - 156s 30ms/step - loss: 0.1523 - acc: 0.9427\n",
      "Epoch 3/20\n",
      "5214/5214 [==============================] - 157s 30ms/step - loss: 0.1108 - acc: 0.9636\n",
      "Epoch 4/20\n",
      "5214/5214 [==============================] - 157s 30ms/step - loss: 0.0964 - acc: 0.9691\n",
      "Epoch 5/20\n",
      "5214/5214 [==============================] - 157s 30ms/step - loss: 0.0865 - acc: 0.9724\n",
      "Epoch 6/20\n",
      "5214/5214 [==============================] - 156s 30ms/step - loss: 0.0850 - acc: 0.9712\n",
      "Epoch 7/20\n",
      "5214/5214 [==============================] - 157s 30ms/step - loss: 0.0737 - acc: 0.9755\n",
      "Epoch 8/20\n",
      "5214/5214 [==============================] - 158s 30ms/step - loss: 0.0691 - acc: 0.9787\n",
      "Epoch 9/20\n",
      "5214/5214 [==============================] - 163s 31ms/step - loss: 0.0645 - acc: 0.9801\n",
      "Epoch 10/20\n",
      "5214/5214 [==============================] - 160s 31ms/step - loss: 0.0593 - acc: 0.9833\n",
      "Epoch 11/20\n",
      "5214/5214 [==============================] - 157s 30ms/step - loss: 0.0561 - acc: 0.9837\n",
      "Epoch 12/20\n",
      "5214/5214 [==============================] - 156s 30ms/step - loss: 0.0531 - acc: 0.9852\n",
      "Epoch 13/20\n",
      "5214/5214 [==============================] - 158s 30ms/step - loss: 0.0515 - acc: 0.9856\n",
      "Epoch 14/20\n",
      "5214/5214 [==============================] - 163s 31ms/step - loss: 0.0490 - acc: 0.9864\n",
      "Epoch 15/20\n",
      "5214/5214 [==============================] - 159s 30ms/step - loss: 0.0468 - acc: 0.9873\n",
      "Epoch 16/20\n",
      "5214/5214 [==============================] - 157s 30ms/step - loss: 0.0445 - acc: 0.9883\n",
      "Epoch 17/20\n",
      "5214/5214 [==============================] - 155s 30ms/step - loss: 0.0432 - acc: 0.9871\n",
      "Epoch 18/20\n",
      "5214/5214 [==============================] - 155s 30ms/step - loss: 0.0408 - acc: 0.9900\n",
      "Epoch 19/20\n",
      "5214/5214 [==============================] - 155s 30ms/step - loss: 0.0408 - acc: 0.9887\n",
      "Epoch 20/20\n",
      "5214/5214 [==============================] - 157s 30ms/step - loss: 0.0390 - acc: 0.9902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15796e150>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y2 = y\n",
    "y_test2 = y_test\n",
    "y2 = to_categorical(y2)\n",
    "y_test2 = to_categorical(y_test2)\n",
    "model.fit(X, y2,\n",
    "          batch_size = 200,\n",
    "          class_weight = 'balanced',\n",
    "          epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7765273311897106\n",
      "true negative: 96\n",
      "false negative: 2\n",
      "true positive: 387\n",
      "false positive: 137\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# binarizing output\n",
    "y_binary_pred = []\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i, 1] >= .5:\n",
    "        y_binary_pred.append(1)\n",
    "    else:\n",
    "        y_binary_pred.append(0)\n",
    "        \n",
    "tn, fp, fn, tp = confusion_matrix(y_test2[:, 1], y_binary_pred).ravel()\n",
    "accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
    "print('accuracy: {}\\ntrue negative: {}\\nfalse negative: {}\\ntrue positive: {}\\nfalse positive: {}'\n",
    "      .format(accuracy, tn, fn, tp, fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs slightly worse than the CNN and SVC when weighted or unweighted, but it overfits and still leaves us with quite a lot of type I error. Type II error, however, is near zero, making the VGG16 model ever so slightly more useful than the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Our models generally performed at around the same level, but the standout successes were the SVC, the CNN, and the VGG16 models. Our primary evaulating metrics are accuracy and Type II errors, which were most optimal in the VGG16 model. Despite success compared to the other models, VGG16 still has far too many Type I errors to make it usable. For future improvements, the first steps would be utilizing class weights in the VGG16 model to solve the class imbalance issues. As more Type II errors (pneumonia cases missed) would occur, the probability threshold for defining a case as pneumonia would be lowered to perhaps .4 instead of .5.\\\n",
    "\\\n",
    "Erroneously classified images would be pulled and passed to doctors to see what our models might commonly be misattributing or just missing.\\\n",
    "\\\n",
    "Of course, a machine learning pneumonia classifier is not the most useful tool, especially since the models require x-rays. More realistically, a model like this could be use in computer aided diagnoses of hard to find ailments that require scans to identify, such as pulmonary embellisms or other clots. If such a program were to be made, the VGG16 model would be the most productive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
